{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.models import resnet50\n",
    "from PIL import Image\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"./dataset/DeepFashion2/\"\n",
    "image_dirs = {\n",
    "    \"train\": os.path.join(dir, \"deepfashion2_original_images/train/image\"),\n",
    "    \"validation\": os.path.join(dir, \"deepfashion2_original_images/validation/image\")\n",
    "}\n",
    "annotation_dirs = {\n",
    "    \"train\": os.path.join(dir, \"deepfashion2_original_images/train/annos\"),\n",
    "    \"validation\": os.path.join(dir, \"deepfashion2_original_images/validation/annos\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train annotations:  53%|█████▎    | 101337/191961 [28:40<25:38, 58.90it/s] \n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.11.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgcodecs\\src\\loadsave.cpp:929: error: (-215:Assertion failed) !_img.empty() in function 'cv::imwrite'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31merror\u001b[39m                                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 44\u001b[39m\n\u001b[32m     42\u001b[39m output_dir = \u001b[33m\"\u001b[39m\u001b[33m./cropped_clothing\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     43\u001b[39m os.makedirs(output_dir, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m train_images = \u001b[43mload_annotations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mannotation_dirs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_dirs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m val_images = load_annotations(annotation_dirs[\u001b[33m\"\u001b[39m\u001b[33mvalidation\u001b[39m\u001b[33m\"\u001b[39m], image_dirs[\u001b[33m\"\u001b[39m\u001b[33mvalidation\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mvalidation\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 36\u001b[39m, in \u001b[36mload_annotations\u001b[39m\u001b[34m(annotation_dir, image_dir, dataset_type)\u001b[39m\n\u001b[32m     34\u001b[39m             cropped_filename = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename.replace(\u001b[33m'\u001b[39m\u001b[33m.json\u001b[39m\u001b[33m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcategory_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.jpg\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     35\u001b[39m             cropped_path = os.path.join(output_dir, cropped_filename)\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m             \u001b[43mcv2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcropped_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcropped_img\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m             cropped_images.append(cropped_path)\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cropped_images\n",
      "\u001b[31merror\u001b[39m: OpenCV(4.11.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgcodecs\\src\\loadsave.cpp:929: error: (-215:Assertion failed) !_img.empty() in function 'cv::imwrite'\n"
     ]
    }
   ],
   "source": [
    "def load_annotations(annotation_dir, image_dir, dataset_type=\"train\"):\n",
    "    \"\"\"\n",
    "    Load annotations, extract bounding boxes, and crop clothing images.\n",
    "    \"\"\"\n",
    "    cropped_images = []\n",
    "    for filename in tqdm(os.listdir(annotation_dir), desc=f\"Processing {dataset_type} annotations\"):\n",
    "        if filename.endswith(\".json\"):\n",
    "            json_path = os.path.join(annotation_dir, filename)\n",
    "            image_path = os.path.join(image_dir, filename.replace(\".json\", \".jpg\"))\n",
    "\n",
    "            if not os.path.exists(image_path):\n",
    "                continue\n",
    "\n",
    "            with open(json_path, \"r\") as f:\n",
    "                annotation = json.load(f)\n",
    "\n",
    "            for item_key, item in annotation.items():\n",
    "                if not isinstance(item, dict) or \"bounding_box\" not in item:\n",
    "                    continue\n",
    "                \n",
    "                bbox = item[\"bounding_box\"]  # [x1, y1, x2, y2]\n",
    "                category_id = item[\"category_id\"]\n",
    "\n",
    "                # Read image\n",
    "                img = cv2.imread(image_path)\n",
    "                if img is None:\n",
    "                    continue\n",
    "\n",
    "                # Crop clothing item\n",
    "                x1, y1, x2, y2 = map(int, bbox)\n",
    "                cropped_img = img[y1:y2, x1:x2]\n",
    "\n",
    "                # Save cropped image\n",
    "                cropped_filename = f\"{dataset_type}_{filename.replace('.json', '')}_{category_id}.jpg\"\n",
    "                cropped_path = os.path.join(output_dir, cropped_filename)\n",
    "                cv2.imwrite(cropped_path, cropped_img)\n",
    "\n",
    "                cropped_images.append(cropped_path)\n",
    "\n",
    "    return cropped_images\n",
    "\n",
    "output_dir = \"./cropped_clothing\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "train_images = load_annotations(annotation_dirs[\"train\"], image_dirs[\"train\"], \"train\")\n",
    "val_images = load_annotations(annotation_dirs[\"validation\"], image_dirs[\"validation\"], \"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionDataset(Dataset):\n",
    "    def __init__(self, image_paths, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = FashionDataset(train_images, transform=transform)\n",
    "val_dataset = FashionDataset(val_images, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet50(pretrained=True)\n",
    "model.fc = torch.nn.Identity()  # Remove classification layer\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_feature_extractor():\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        for images in train_loader:\n",
    "            images = images.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, torch.zeros_like(outputs))  # Dummy loss for feature learning\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{EPOCHS}], Loss: {loss.item()}\")\n",
    "\n",
    "train_feature_extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"fashion_feature_extractor.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "approach 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"fashion_feature_extractor.pth\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 2048\n",
    "faiss_index = faiss.IndexFlatL2(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(image_paths):\n",
    "    features = []\n",
    "    for img_path in tqdm(image_paths, desc=\"Extracting Features\"):\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = transform(image).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            feature = model(image).cpu().numpy()\n",
    "        \n",
    "        features.append(feature)\n",
    "\n",
    "    return np.vstack(features)\n",
    "\n",
    "feature_vectors = extract_features(train_images + val_images)\n",
    "faiss_index.add(feature_vectors)\n",
    "faiss.write_index(faiss_index, \"fashion_faiss.index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_images_dir = \"/path/to/user/uploads\"\n",
    "user_images = [os.path.join(user_images_dir, f) for f in os.listdir(user_images_dir) if f.endswith(\".jpg\")]\n",
    "user_features = extract_features(user_images)\n",
    "faiss_index.add(user_features)\n",
    "faiss.write_index(faiss_index, \"fashion_faiss.index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "approach 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import faiss\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_images_dir = \"./dataset/DeepFashion2/deepfashion2_original_images/test/test/image\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", DEVICE)\n",
    "model = resnet50(pretrained=True)\n",
    "model.fc = torch.nn.Identity()\n",
    "model.load_state_dict(torch.load(\"fashion_feature_extractor.pth\"))\n",
    "model = model.to(DEVICE)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 2048\n",
    "faiss_index = faiss.IndexFlatL2(d)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "user_images = [os.path.join(user_images_dir, img) for img in os.listdir(user_images_dir) if img.endswith(('.jpg', '.png'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(image_paths):\n",
    "    features = []\n",
    "    for img_path in tqdm(image_paths, desc=\"Extracting Features from User Images\"):\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = transform(image).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            feature = model(image).cpu().numpy()\n",
    "\n",
    "        features.append(feature)\n",
    "\n",
    "    return np.vstack(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if user_images:\n",
    "    user_feature_vectors = extract_features(user_images)\n",
    "    faiss_index.add(user_feature_vectors)\n",
    "\n",
    "    faiss.write_index(faiss_index, \"user_images_faiss.index\")\n",
    "\n",
    "print(f\"Stored {len(user_images)} user-uploaded images in FAISS.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_images(uploaded_image_path, top_k=5):\n",
    "    image = Image.open(uploaded_image_path).convert(\"RGB\")\n",
    "    image = transform(image).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        feature = model(image).cpu().numpy()\n",
    "\n",
    "    faiss_index = faiss.read_index(\"user_images_faiss.index\")\n",
    "\n",
    "    distances, indices = faiss_index.search(feature, top_k)\n",
    "\n",
    "    return [user_images[i] for i in indices[0]]\n",
    "\n",
    "uploaded_image = \"/path/to/new_user_uploaded_image.jpg\"\n",
    "similar_images = find_similar_images(uploaded_image)\n",
    "\n",
    "print(\"Top Similar User-Uploaded Images:\", similar_images)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
