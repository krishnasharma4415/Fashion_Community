{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.models import resnet50\n",
    "from PIL import Image\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"./dataset/DeepFashion2/\"\n",
    "image_dirs = {\n",
    "    \"train\": os.path.join(dir, \"deepfashion2_original_images/train/image\"),\n",
    "    \"validation\": os.path.join(dir, \"deepfashion2_original_images/validation/image\")\n",
    "}\n",
    "annotation_dirs = {\n",
    "    \"train\": os.path.join(dir, \"deepfashion2_original_images/train/annos\"),\n",
    "    \"validation\": os.path.join(dir, \"deepfashion2_original_images/validation/annos\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_annotations(annotation_dir, image_dir, dataset_type=\"train\"):\n",
    "    \"\"\"\n",
    "    Load annotations, extract bounding boxes, and crop clothing images.\n",
    "    \"\"\"\n",
    "    cropped_images = []\n",
    "    for filename in tqdm(os.listdir(annotation_dir), desc=f\"Processing {dataset_type} annotations\"):\n",
    "        if filename.endswith(\".json\"):\n",
    "            json_path = os.path.join(annotation_dir, filename)\n",
    "            image_path = os.path.join(image_dir, filename.replace(\".json\", \".jpg\"))\n",
    "\n",
    "            if not os.path.exists(image_path):\n",
    "                continue\n",
    "\n",
    "            with open(json_path, \"r\") as f:\n",
    "                annotation = json.load(f)\n",
    "\n",
    "            for item_key, item in annotation.items():\n",
    "                if not isinstance(item, dict) or \"bounding_box\" not in item:\n",
    "                    continue\n",
    "                \n",
    "                bbox = item[\"bounding_box\"] \n",
    "                category_id = item[\"category_id\"]\n",
    "\n",
    "                img = cv2.imread(image_path)\n",
    "                if img is None:\n",
    "                    continue\n",
    "\n",
    "                x1, y1, x2, y2 = map(int, bbox)\n",
    "                cropped_img = img[y1:y2, x1:x2]\n",
    "\n",
    "                cropped_filename = f\"{dataset_type}_{filename.replace('.json', '')}_{category_id}.jpg\"\n",
    "                cropped_path = os.path.join(output_dir, cropped_filename)\n",
    "                cv2.imwrite(cropped_path, cropped_img)\n",
    "\n",
    "                cropped_images.append(cropped_path)\n",
    "\n",
    "    return cropped_images\n",
    "\n",
    "output_dir = \"./cropped_clothing\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "train_images = load_annotations(annotation_dirs[\"train\"], image_dirs[\"train\"], \"train\")\n",
    "val_images = load_annotations(annotation_dirs[\"validation\"], image_dirs[\"validation\"], \"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionDataset(Dataset):\n",
    "    def __init__(self, image_paths, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = FashionDataset(train_images, transform=transform)\n",
    "val_dataset = FashionDataset(val_images, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet50(pretrained=True)\n",
    "model.fc = torch.nn.Identity() \n",
    "model = model.to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_feature_extractor():\n",
    "    model.train()\n",
    "    for epoch in range(EPOCHS):\n",
    "        for images in train_loader:\n",
    "            images = images.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, torch.zeros_like(outputs))  \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{EPOCHS}], Loss: {loss.item()}\")\n",
    "\n",
    "train_feature_extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"fashion_feature_extractor.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "approach 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"fashion_feature_extractor.pth\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 2048\n",
    "faiss_index = faiss.IndexFlatL2(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(image_paths):\n",
    "    features = []\n",
    "    for img_path in tqdm(image_paths, desc=\"Extracting Features\"):\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = transform(image).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            feature = model(image).cpu().numpy()\n",
    "        \n",
    "        features.append(feature)\n",
    "\n",
    "    return np.vstack(features)\n",
    "\n",
    "feature_vectors = extract_features(train_images + val_images)\n",
    "faiss_index.add(feature_vectors)\n",
    "faiss.write_index(faiss_index, \"fashion_faiss.index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_images_dir = \"/path/to/user/uploads\"\n",
    "user_images = [os.path.join(user_images_dir, f) for f in os.listdir(user_images_dir) if f.endswith(\".jpg\")]\n",
    "user_features = extract_features(user_images)\n",
    "faiss_index.add(user_features)\n",
    "faiss.write_index(faiss_index, \"fashion_faiss.index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "approach 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import faiss\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_images_dir = \"./dataset/DeepFashion2/deepfashion2_original_images/test/test/image\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", DEVICE)\n",
    "model = resnet50(pretrained=True)\n",
    "model.fc = torch.nn.Identity()\n",
    "model.load_state_dict(torch.load(\"fashion_feature_extractor.pth\"))\n",
    "model = model.to(DEVICE)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 2048\n",
    "faiss_index = faiss.IndexFlatL2(d)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "user_images = [os.path.join(user_images_dir, img) for img in os.listdir(user_images_dir) if img.endswith(('.jpg', '.png'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(image_paths):\n",
    "    features = []\n",
    "    for img_path in tqdm(image_paths, desc=\"Extracting Features from User Images\"):\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = transform(image).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            feature = model(image).cpu().numpy()\n",
    "\n",
    "        features.append(feature)\n",
    "\n",
    "    return np.vstack(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if user_images:\n",
    "    user_feature_vectors = extract_features(user_images)\n",
    "    faiss_index.add(user_feature_vectors)\n",
    "\n",
    "    faiss.write_index(faiss_index, \"user_images_faiss.index\")\n",
    "\n",
    "print(f\"Stored {len(user_images)} user-uploaded images in FAISS.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_images(uploaded_image_path, top_k=5):\n",
    "    image = Image.open(uploaded_image_path).convert(\"RGB\")\n",
    "    image = transform(image).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        feature = model(image).cpu().numpy()\n",
    "\n",
    "    faiss_index = faiss.read_index(\"user_images_faiss.index\")\n",
    "\n",
    "    distances, indices = faiss_index.search(feature, top_k)\n",
    "\n",
    "    return [user_images[i] for i in indices[0]]\n",
    "\n",
    "uploaded_image = \"/path/to/new_user_uploaded_image.jpg\"\n",
    "similar_images = find_similar_images(uploaded_image)\n",
    "\n",
    "print(\"Top Similar User-Uploaded Images:\", similar_images)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
